{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Approaches To NLP Problems\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/codecademy-content/courses/NLP/Natural_Language_Processing_Overview.gif\" height=\"500\" width=\"500\">\n",
    "[Image Source](https://s3.amazonaws.com/codecademy-content/courses/NLP/Natural_Language_Processing_Overview.gif)\n",
    "\n",
    "# CONTEXT \n",
    "* [Importing packages and Reading Data](#1)\n",
    "* [Data Preprocessing](#2)\n",
    "    * [Text Cleaning](#2.1)\n",
    "    * [Tokenizer](#2.2)\n",
    "    * [Remove StopWord](#2.3)\n",
    "    * [Token normalization](#2.4)\n",
    "    * [Transforming tokens to a vector](#2.5)\n",
    "* [Building Model](#3)\n",
    "    * [Logistic Regression](#3.1)\n",
    "    * [SVC](#3.2)\n",
    "    * [MultinomialNB](#3.3)\n",
    "    * [XGBoost](#3.4)\n",
    "* [Grid Search](#4)\n",
    "* [GloVe](#5)\n",
    "* [Deep Learning](#6)\n",
    "    * [Sequential Neural Net](#6.1) \n",
    "    * [LSTM](#6.2)\n",
    "    * [GRU](#6.3)\n",
    "* [References](#7)\n",
    "\n",
    "\n",
    "# Intro\n",
    "In this notebook, I just want to try different approaches, methods and modeles to solve NLP problems. We are going to start with very basic model and feature engineering and then improve it using different other models. We will also use deep neural networks and see how its perform compare to others. Last but not least we will try emsembling.\n",
    "\n",
    "# Data\n",
    "**Each sample in the train and test set has the following information:**\n",
    "\n",
    "* The text of a tweet\n",
    "* A keyword from that tweet (although this may be blank!)\n",
    "* The location the tweet was sent from (may also be blank)\n",
    "\n",
    "# What am I predicting?\n",
    "You are predicting whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.\n",
    "\n",
    "**Files**\n",
    "* train.csv - the training set\n",
    "* test.csv - the test set\n",
    "* sample_submission.csv - a sample submission file in the correct format\n",
    "**Columns**\n",
    "* id - a unique identifier for each tweet\n",
    "* text - the text of the tweet\n",
    "* location - the location the tweet was sent from (may be blank)\n",
    "* keyword - a particular keyword from the tweet (may be blank)\n",
    "* target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing packages and Reading Data <a id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Importing libraries\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "\"\"\"Let's load the data files\"\"\"\n",
    "train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n",
    "sub = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Reading train data\"\"\"\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3263, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Reading test data\"\"\"\n",
    "print(test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       0\n",
       "1   2       0\n",
       "2   3       0\n",
       "3   9       0\n",
       "4  11       0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"reading submission file\"\"\"\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing <a id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = train_test_split(train.text, \n",
    "                                                  train.target,\n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6851,)\n",
      "(762,)\n"
     ]
    }
   ],
   "source": [
    "print(xtrain.shape)\n",
    "print(xvalid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.1 Text Cleaning <a id=\"2.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 465 ms, sys: 803 µs, total: 466 ms\n",
      "Wall time: 464 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4620    mcfadden reportedly to test hamstring thursday...\n",
       "2858       w nema warns nigerians to prepare for drought \n",
       "3098    when i was cooking earlier i got electrocuted ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Applying the cleaning function to both test and training datasets\n",
    "xtrain = xtrain.apply(lambda x: clean_text(x))\n",
    "xvalid = xvalid.apply(lambda x: clean_text(x))\n",
    "xtrain.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Tokenizer <a id=\"2.2\"></a>\n",
    "[Documentation](https://www.nltk.org/api/nltk.tokenize.html****)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 862 ms, sys: 4.12 ms, total: 866 ms\n",
      "Wall time: 865 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4620    [mcfadden, reportedly, to, test, hamstring, th...\n",
       "2858    [w, nema, warns, nigerians, to, prepare, for, ...\n",
       "3098    [when, i, was, cooking, earlier, i, got, elect...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer1 = nltk.tokenize.WhitespaceTokenizer()\n",
    "tokenizer2 = nltk.tokenize.TreebankWordTokenizer()\n",
    "tokenizer3 = nltk.tokenize.WordPunctTokenizer()\n",
    "tokenizer4 = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "tokenizer5 = nltk.tokenize.TweetTokenizer()\n",
    "\n",
    "# appling tokenizer5\n",
    "xtrain = xtrain.apply(lambda x: tokenizer5.tokenize(x))\n",
    "xvalid = xvalid.apply(lambda x: tokenizer5.tokenize(x))\n",
    "xtrain.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Remove StopWord <a id=\"2.3\"></a>\n",
    "[Documentation](https://www.geeksforgeeks.org/removing-stop-words-nltk-python/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 s, sys: 1.57 s, total: 17.6 s\n",
      "Wall time: 17.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Removing stopwords belonging to english language\n",
    "    \n",
    "    \"\"\"\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return words\n",
    "\n",
    "\n",
    "xtrain = xtrain.apply(lambda x : remove_stopwords(x))\n",
    "xvalid = xvalid.apply(lambda x : remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11 ms, sys: 1.03 ms, total: 12.1 ms\n",
      "Wall time: 11.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def combine_text(list_of_text):\n",
    "    combined_text = ' '.join(list_of_text)\n",
    "    return combined_text\n",
    "\n",
    "xtrain = xtrain.apply(lambda x : combine_text(x))\n",
    "xvalid = xvalid.apply(lambda x : combine_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Token normalization <a id=\"2.4\"></a>\n",
    "Token normalisation means converting different tokens to their base forms. This can be done either by:\n",
    "\n",
    "* Stemming : removing and replacing suffixes to get to the root form of the word, which is called the stem for instance cats - cat, wolves - wolv\n",
    "* Lemmatization : Returns the base or dictionary form of a word, which is known as the lemma\n",
    "\n",
    "[source](https://www.google.com/search?q=not+least+but+last&oq=not+least&aqs=chrome.7.69i57j0l7.22351j0j1&sourceid=chrome&ie=UTF-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemmer\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "# Lemmatizer\n",
    "lemmatizer=nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# Appling Lemmatizer\n",
    "xtrain = xtrain.apply(lambda x: lemmatizer.lemmatize(x))\n",
    "xvalid = xvalid.apply(lambda x: lemmatizer.lemmatize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Transforming tokens to a vector <a id=\"2.5\"></a>\n",
    "[Countvectorizer Features](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "\n",
    "[TFIDF Features](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "\n",
    "*[Reading](http://www.tfidf.com/)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appling CountVectorizer()\n",
    "count_vectorizer = CountVectorizer()\n",
    "xtrain_vectors = count_vectorizer.fit_transform(xtrain)\n",
    "xvalid_vectors = count_vectorizer.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appling TFIDF\n",
    "tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2), norm='l2')\n",
    "xtrain_tfidf = tfidf.fit_transform(xtrain)\n",
    "xvalid_tfidf = tfidf.transform(xvalid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Building Models <a id=\"3\"></a>\n",
    "\n",
    "## Logistic Regression <a id=\"3.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple Logistic Regression on TFIDF\n",
      "f1_score : 0.72697\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Logistic Regression on TFIDF\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_tfidf, ytrain)\n",
    "#scores = model_selection.cross_val_score(clf, train_tfidf, ytrain, cv=5, scoring=\"f1\")\n",
    "\n",
    "predictions = clf.predict(xvalid_tfidf)\n",
    "print('simple Logistic Regression on TFIDF')\n",
    "print (\"f1_score :\", np.round(f1_score(yvalid, predictions),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple Logistic Regression on CountVectorizer\n",
      "f1_score : 0.73515\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Logistic Regression on CountVec\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_vectors, ytrain)\n",
    "#scores = model_selection.cross_val_score(clf, xtrain_vectors, ytrain_vectors, cv=5, scoring=\"f1\")\n",
    "\n",
    "predictions = clf.predict(xvalid_vectors)\n",
    "print('simple Logistic Regression on CountVectorizer')\n",
    "print (\"f1_score :\", np.round(f1_score(yvalid, predictions),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm!! We just improved our first model by 0.01.\n",
    "\n",
    "But, we can find different scores by playing with the parameters of count, tfidf and model.\n",
    "\n",
    "[about f1_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)\n",
    "\n",
    "Let's try SVC and Naives Bayes Classifier\n",
    "\n",
    "## SVC <a id=\"3.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC on TFIDF\n",
      "f1_score : 0.7155\n"
     ]
    }
   ],
   "source": [
    "# Fitting a LinearSVC on TFIDF\n",
    "clf = SVC()\n",
    "clf.fit(xtrain_tfidf, ytrain)\n",
    "\n",
    "predictions = clf.predict(xvalid_tfidf)\n",
    "print('SVC on TFIDF')\n",
    "print (\"f1_score :\", np.round(f1_score(yvalid, predictions),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC on CountVectorizer\n",
      "f1_score : 0.72289\n"
     ]
    }
   ],
   "source": [
    "# Fitting a LinearSVC on CountVec\n",
    "clf = SVC()\n",
    "clf.fit(xtrain_vectors, ytrain)\n",
    "\n",
    "predictions = clf.predict(xvalid_vectors)\n",
    "print('SVC on CountVectorizer')\n",
    "print (\"f1_score :\", np.round(f1_score(yvalid, predictions),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bad performance by svc on this dataset!!\n",
    "\n",
    "## MultinomialNB <a id=\"3.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB on TFIDF\n",
      "f1_score : 0.70486\n"
     ]
    }
   ],
   "source": [
    "# Fitting a MultinomialNB on TFIDF\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_tfidf, ytrain)\n",
    "\n",
    "predictions = clf.predict(xvalid_tfidf)\n",
    "print('MultinomialNB on TFIDF')\n",
    "print (\"f1_score :\", np.round(f1_score(yvalid, predictions),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB on CountVectorizer\n",
      "f1_score : 0.73077\n"
     ]
    }
   ],
   "source": [
    "# Fitting a MultinomialNB on CountVec\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_vectors, ytrain)\n",
    "\n",
    "predictions = clf.predict(xvalid_vectors)\n",
    "print('MultinomialNB on CountVectorizer')\n",
    "print (\"f1_score :\", np.round(f1_score(yvalid, predictions),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good!! but the logistic regression on counts is still better than other ancient models we try here. Let's jump into XGBoost model.\n",
    "\n",
    "## XGBoost <a id=\"3.4\"></a>\n",
    "\n",
    "[Parameters](https://xgboost.readthedocs.io/en/latest/parameter.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier on TFIDF\n",
      "f1_score : 0.68151\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on TFIDF\n",
    "clf = xgb.XGBClassifier(max_depth=5, n_estimators=300, colsample_bytree=0.8, \n",
    "                        subsample=0.5, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_tfidf.tocsc(), ytrain)\n",
    "predictions = clf.predict(xvalid_tfidf.tocsc())\n",
    "\n",
    "print('XGBClassifier on TFIDF')\n",
    "print (\"f1_score :\", np.round(f1_score(yvalid, predictions),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier on CountVectorizer\n",
      "f1_score : 0.68814\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on CountVec\n",
    "clf = xgb.XGBClassifier(max_depth=5, n_estimators=300, colsample_bytree=0.8, \n",
    "                        subsample=0.5, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_vectors, ytrain)\n",
    "\n",
    "predictions = clf.predict(xvalid_vectors)\n",
    "print('XGBClassifier on CountVectorizer')\n",
    "print (\"f1_score :\", np.round(f1_score(yvalid, predictions),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seens like XGBoost perform worst than other! but that is not correct. I haven't done any hyperparameter optimizations yet. Let's do it.\n",
    "\n",
    "# Grid Search <a id=\"4\"></a>\n",
    "Now let's add Grid Search to all the models with the hopes of optimizing their hyperparameters and thus improving their accuracy. Are the default model parameters the best bet? Let's find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create a function to tune hyperparameters of the selected models.'''\n",
    "seed = 44\n",
    "def grid_search_cv(model, params):\n",
    "    global best_params, best_score\n",
    "    grid_search = GridSearchCV(estimator = model, param_grid = params, cv = 5, \n",
    "                             verbose = 3,\n",
    "                             scoring = 'f1', n_jobs = -1)\n",
    "    grid_search.fit(xtrain_vectors, ytrain)\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    \n",
    "    return best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    4.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR best params:{'C': 1.2589254117941673, 'penalty': 'l2'} & best_score:0.75323\n"
     ]
    }
   ],
   "source": [
    "'''Define hyperparameters of Logistic Regression.'''\n",
    "LR_model = LogisticRegression()\n",
    "\n",
    "LR_params = {'penalty':['l1', 'l2'],\n",
    "             'C': np.logspace(0.1, 1, 4, 8 ,10)}\n",
    "\n",
    "grid_search_cv(LR_model, LR_params)\n",
    "LR_best_params, LR_best_score = best_params, best_score\n",
    "print('LR best params:{} & best_score:{:0.5f}' .format(LR_best_params, LR_best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   41.0s\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  4.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC best params:{'C': 2.51188643150958, 'kernel': 'rbf'} & best_score:0.74863\n"
     ]
    }
   ],
   "source": [
    "'''Define hyperparameters of Logistic Regression.'''\n",
    "SVC_model = SVC()\n",
    "\n",
    "SVC_params = {'kernel':[ 'linear', 'rbf', 'sigmoid'],\n",
    "             'C': np.logspace(0.1, 1,10)}\n",
    "\n",
    "grid_search_cv(SVC_model, SVC_params)\n",
    "SVC_best_params, SVC_best_score = best_params, best_score\n",
    "print('SVC best params:{} & best_score:{:0.5f}' .format(SVC_best_params, SVC_best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB best params:{'alpha': 1} & best_score:0.75783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "'''Define hyperparameters of Logistic Regression.'''\n",
    "NB_model = MultinomialNB()\n",
    "\n",
    "NB_params = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "grid_search_cv(NB_model, NB_params)\n",
    "NB_best_params, NB_best_score = best_params, best_score\n",
    "print('NB best params:{} & best_score:{:0.5f}' .format(NB_best_params, NB_best_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After hyperparameter tuning, we can see that countvector data give us improved result. Before our best score was **0.73515 (LR)** and now after gridsearch our score is **0.75783 (NB)**.\n",
    "\n",
    "I am not optimizing XGBoost hyperparameters becuase its  is time consuming. but you can try optimization on all the models for better score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''For XGBC, the following hyperparameters are usually tunned.'''\n",
    "#'''https://xgboost.readthedocs.io/en/latest/parameter.html'''\n",
    "\n",
    "#XGB_model = XGBClassifier(\n",
    "#            n_estimators=500,\n",
    "#            verbose = True)\n",
    "\n",
    "\n",
    "#XGB_params = {'max_depth': (2, 5),\n",
    "#               'reg_alpha':  (0.01, 0.4),\n",
    "#               'reg_lambda': (0.01, 0.4),\n",
    "#               'learning_rate': (0.1, 0.4),\n",
    "#               'colsample_bytree': (0.3, 1),\n",
    "#               'gamma': (0.01, 0.7),\n",
    "#               'num_leaves': (2, 5),\n",
    "#               'min_child_samples': (1, 5),\n",
    "#              'subsample': [0.5, 0.8],\n",
    "#              'random_state':[seed]}\n",
    "\n",
    "#grid_search_cv(XGB_model, XGB_params)\n",
    "#XGB_best_params, XGB_best_score = best_params, best_score\n",
    "#print('XGB best params:{} & best_score:{:0.5f}' .format(XGB_best_params, XGB_best_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe for Vectorization <a id=\"5\"></a>\n",
    "Here we will use GloVe pretrained corpus model to represent our words.It is available in 3 varieties :50D ,100D and 200 Dimentional.We will try 200 D here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Load the Glove vectors in a dictionay\"\"\"\n",
    "embeddings_index={}\n",
    "with open('../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt','r') as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word=values[0]\n",
    "        vectors=np.asarray(values[1:],'float32')\n",
    "        embeddings_index[word]=vectors\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Function Creates a normalized vector for the whole sentence\"\"\"\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stopwords.words('english')]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(200)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6851/6851 [00:13<00:00, 498.14it/s]\n",
      "100%|██████████| 762/762 [00:01<00:00, 473.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# create sentence vectors using the above function for training and validation set\n",
    "# create glove features\n",
    "xtrain_glove = np.array([sent2vec(x) for x in tqdm(xtrain)])\n",
    "xvalid_glove = np.array([sent2vec(x) for x in tqdm(xvalid)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6851, 200), (762, 200))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of data after embedding\n",
    "xtrain_glove.shape,  xvalid_glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier on GloVe featur\n",
      "f1_score : 0.74799\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on glove features\n",
    "clf = xgb.XGBClassifier(max_depth=8, n_estimators=300, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_glove, ytrain)\n",
    "\n",
    "predictions = clf.predict(xvalid_glove)\n",
    "print('XGBClassifier on GloVe featur')\n",
    "print (\"f1_score :\", np.round(f1_score(yvalid, predictions),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we compare the previous xgboost results with XGBClassifier on GloVe feature result we can see that score has been increase by 0.06. we can further improve by tuning of parameters.\n",
    "\n",
    "# Deep Learning <a id=\"6\"></a>\n",
    "\n",
    "## Sequential Neural Net <a id=\"6.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"scale the data before any neural net\"\"\"\n",
    "scl = preprocessing.StandardScaler()\n",
    "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "xvalid_glove_scl = scl.transform(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"create a simple 2 layer sequential neural net\"\"\"\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(200, input_dim=200, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6851 samples, validate on 762 samples\n",
      "Epoch 1/10\n",
      "6851/6851 [==============================] - 1s 211us/step - loss: 0.5420 - accuracy: 0.7530 - val_loss: 0.4603 - val_accuracy: 0.7927\n",
      "Epoch 2/10\n",
      "6851/6851 [==============================] - 1s 74us/step - loss: 0.4395 - accuracy: 0.8043 - val_loss: 0.4490 - val_accuracy: 0.8084\n",
      "Epoch 3/10\n",
      "6851/6851 [==============================] - 0s 71us/step - loss: 0.4130 - accuracy: 0.8224 - val_loss: 0.4640 - val_accuracy: 0.7900\n",
      "Epoch 4/10\n",
      "6851/6851 [==============================] - 1s 76us/step - loss: 0.3839 - accuracy: 0.8326 - val_loss: 0.4518 - val_accuracy: 0.8031\n",
      "Epoch 5/10\n",
      "6851/6851 [==============================] - 0s 72us/step - loss: 0.3728 - accuracy: 0.8421 - val_loss: 0.4612 - val_accuracy: 0.8045\n",
      "Epoch 6/10\n",
      "6851/6851 [==============================] - 1s 75us/step - loss: 0.3569 - accuracy: 0.8476 - val_loss: 0.4705 - val_accuracy: 0.8018\n",
      "Epoch 7/10\n",
      "6851/6851 [==============================] - 0s 73us/step - loss: 0.3275 - accuracy: 0.8600 - val_loss: 0.4854 - val_accuracy: 0.7874\n",
      "Epoch 8/10\n",
      "6851/6851 [==============================] - 1s 75us/step - loss: 0.3263 - accuracy: 0.8583 - val_loss: 0.4901 - val_accuracy: 0.7756\n",
      "Epoch 9/10\n",
      "6851/6851 [==============================] - 1s 76us/step - loss: 0.3070 - accuracy: 0.8673 - val_loss: 0.5136 - val_accuracy: 0.7717\n",
      "Epoch 10/10\n",
      "6851/6851 [==============================] - 1s 79us/step - loss: 0.2892 - accuracy: 0.8794 - val_loss: 0.5192 - val_accuracy: 0.7743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f3cd0042f60>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_glove_scl, y=ytrain, batch_size=64, \n",
    "          epochs=10, verbose=1, \n",
    "          validation_data=(xvalid_glove_scl, yvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 layer sequential neural net on GloVe Feature\n",
      "f1_score : 0.73375\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(xvalid_glove_scl)\n",
    "predictions = np.round(predictions).astype(int)\n",
    "print('2 layer sequential neural net on GloVe Feature')\n",
    "print (\"f1_score :\", np.round(f1_score(yvalid, predictions),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!! 2 layer sequential neural net on GloVe Feature perform better than xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM <a id=\"6.2\"></a>\n",
    "For LSTM modeling we need to tokensize the text data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 16446\n"
     ]
    }
   ],
   "source": [
    "# using keras tokenizer here\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 80\n",
    "\n",
    "token.fit_on_texts(list(xtrain) + list(xvalid))\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xvalid_seq = token.texts_to_sequences(xvalid)\n",
    "\n",
    "# zero pad the sequences\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index\n",
    "print('Number of unique words:',len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16446/16446 [00:00<00:00, 210104.21it/s]\n"
     ]
    }
   ],
   "source": [
    "#create an embedding matrix for the words we have in the dataset\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 200))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     200,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6851 samples, validate on 762 samples\n",
      "Epoch 1/100\n",
      "6851/6851 [==============================] - 9s 1ms/step - loss: 0.6724 - accuracy: 0.5919 - val_loss: 0.5744 - val_accuracy: 0.7651\n",
      "Epoch 2/100\n",
      "6851/6851 [==============================] - 8s 1ms/step - loss: 0.5471 - accuracy: 0.7386 - val_loss: 0.4780 - val_accuracy: 0.8005\n",
      "Epoch 3/100\n",
      "6851/6851 [==============================] - 9s 1ms/step - loss: 0.5125 - accuracy: 0.7613 - val_loss: 0.4421 - val_accuracy: 0.8110\n",
      "Epoch 4/100\n",
      "6851/6851 [==============================] - 8s 1ms/step - loss: 0.4895 - accuracy: 0.7816 - val_loss: 0.4318 - val_accuracy: 0.8136\n",
      "Epoch 5/100\n",
      "6851/6851 [==============================] - 8s 1ms/step - loss: 0.4844 - accuracy: 0.7809 - val_loss: 0.4276 - val_accuracy: 0.8123\n",
      "Epoch 6/100\n",
      "6851/6851 [==============================] - 8s 1ms/step - loss: 0.4738 - accuracy: 0.7930 - val_loss: 0.4268 - val_accuracy: 0.8136\n",
      "Epoch 7/100\n",
      "6851/6851 [==============================] - 8s 1ms/step - loss: 0.4663 - accuracy: 0.7939 - val_loss: 0.4272 - val_accuracy: 0.8110\n",
      "Epoch 8/100\n",
      "6851/6851 [==============================] - 8s 1ms/step - loss: 0.4581 - accuracy: 0.7959 - val_loss: 0.4264 - val_accuracy: 0.8071\n",
      "Epoch 9/100\n",
      "6851/6851 [==============================] - 8s 1ms/step - loss: 0.4665 - accuracy: 0.7908 - val_loss: 0.4266 - val_accuracy: 0.8084\n",
      "Epoch 10/100\n",
      "6851/6851 [==============================] - 9s 1ms/step - loss: 0.4621 - accuracy: 0.7983 - val_loss: 0.4266 - val_accuracy: 0.8071\n",
      "Epoch 11/100\n",
      "6851/6851 [==============================] - 8s 1ms/step - loss: 0.4562 - accuracy: 0.7964 - val_loss: 0.4243 - val_accuracy: 0.8071\n",
      "Epoch 12/100\n",
      "6851/6851 [==============================] - 8s 1ms/step - loss: 0.4468 - accuracy: 0.8013 - val_loss: 0.4218 - val_accuracy: 0.8136\n",
      "Epoch 13/100\n",
      "6851/6851 [==============================] - 8s 1ms/step - loss: 0.4432 - accuracy: 0.8022 - val_loss: 0.4222 - val_accuracy: 0.8097\n",
      "Epoch 14/100\n",
      "6851/6851 [==============================] - 8s 1ms/step - loss: 0.4445 - accuracy: 0.8054 - val_loss: 0.4234 - val_accuracy: 0.8045\n",
      "Epoch 15/100\n",
      "6851/6851 [==============================] - 8s 1ms/step - loss: 0.4414 - accuracy: 0.8051 - val_loss: 0.4282 - val_accuracy: 0.8005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f3cc02e6f60>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "\n",
    "model.fit(xtrain_pad, y=ytrain, batch_size=512, epochs=100, verbose=1, validation_data=(xvalid_pad, yvalid), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple LSTM\n",
      "f1_score : 0.74667\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(xvalid_pad)\n",
    "predictions = np.round(predictions).astype(int)\n",
    "\n",
    "print('simple LSTM')\n",
    "print (\"f1_score :\", np.round(f1_score(yvalid, predictions),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Waoh!! LSTM model perform as expected. Best score till now!!.\n",
    "\n",
    "## GRU <a id=\"6.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     200,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(GRU(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6851 samples, validate on 762 samples\n",
      "Epoch 1/100\n",
      "6851/6851 [==============================] - 9s 1ms/step - loss: 0.6881 - accuracy: 0.5769 - val_loss: 0.6061 - val_accuracy: 0.6982\n",
      "Epoch 2/100\n",
      "6851/6851 [==============================] - 8s 1ms/step - loss: 0.5826 - accuracy: 0.7057 - val_loss: 0.5382 - val_accuracy: 0.7467\n",
      "Epoch 3/100\n",
      "6851/6851 [==============================] - 8s 1ms/step - loss: 0.5448 - accuracy: 0.7453 - val_loss: 0.5441 - val_accuracy: 0.7664\n",
      "Epoch 4/100\n",
      "6851/6851 [==============================] - 8s 1ms/step - loss: 0.5257 - accuracy: 0.7543 - val_loss: 0.5304 - val_accuracy: 0.7743\n",
      "Epoch 5/100\n",
      "6851/6851 [==============================] - 8s 1ms/step - loss: 0.5018 - accuracy: 0.7767 - val_loss: 0.5261 - val_accuracy: 0.7717\n",
      "Epoch 6/100\n",
      "6851/6851 [==============================] - 8s 1ms/step - loss: 0.4945 - accuracy: 0.7675 - val_loss: 0.5272 - val_accuracy: 0.7822\n",
      "Epoch 7/100\n",
      "6851/6851 [==============================] - 8s 1ms/step - loss: 0.4921 - accuracy: 0.7771 - val_loss: 0.5253 - val_accuracy: 0.7677\n",
      "Epoch 8/100\n",
      "6851/6851 [==============================] - 8s 1ms/step - loss: 0.4923 - accuracy: 0.7799 - val_loss: 0.5322 - val_accuracy: 0.7612\n",
      "Epoch 9/100\n",
      "6851/6851 [==============================] - 8s 1ms/step - loss: 0.4846 - accuracy: 0.7815 - val_loss: 0.5179 - val_accuracy: 0.7677\n",
      "Epoch 10/100\n",
      "6851/6851 [==============================] - 9s 1ms/step - loss: 0.4795 - accuracy: 0.7872 - val_loss: 0.5227 - val_accuracy: 0.7690\n",
      "Epoch 11/100\n",
      "6851/6851 [==============================] - 8s 1ms/step - loss: 0.4792 - accuracy: 0.7889 - val_loss: 0.5177 - val_accuracy: 0.7730\n",
      "Epoch 12/100\n",
      "6851/6851 [==============================] - 8s 1ms/step - loss: 0.4777 - accuracy: 0.7869 - val_loss: 0.5157 - val_accuracy: 0.7638\n",
      "Epoch 13/100\n",
      "6851/6851 [==============================] - 8s 1ms/step - loss: 0.4766 - accuracy: 0.7916 - val_loss: 0.5223 - val_accuracy: 0.7677\n",
      "Epoch 14/100\n",
      "6851/6851 [==============================] - 8s 1ms/step - loss: 0.4605 - accuracy: 0.8025 - val_loss: 0.5069 - val_accuracy: 0.7677\n",
      "Epoch 15/100\n",
      "6851/6851 [==============================] - 8s 1ms/step - loss: 0.4616 - accuracy: 0.7921 - val_loss: 0.5164 - val_accuracy: 0.7677\n",
      "Epoch 16/100\n",
      "6851/6851 [==============================] - 8s 1ms/step - loss: 0.4641 - accuracy: 0.7954 - val_loss: 0.5191 - val_accuracy: 0.7690\n",
      "Epoch 17/100\n",
      "6851/6851 [==============================] - 8s 1ms/step - loss: 0.4620 - accuracy: 0.7920 - val_loss: 0.5125 - val_accuracy: 0.7690\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f3c91cd4e80>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model with early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "\n",
    "model.fit(xtrain_pad, y=ytrain, batch_size=512, epochs=100, verbose=1, validation_data=(xvalid_pad, yvalid), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple GRU\n",
      "f1_score : 0.71429\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(xvalid_pad)\n",
    "predictions = np.round(predictions).astype(int)\n",
    "print('simple GRU')\n",
    "print (\"f1_score :\", np.round(f1_score(yvalid, predictions),5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References <a id=\"7\"></a>\n",
    "1. https://www.kaggle.com/vikassingh1996/simple-model-feat-nlp-disaster-tweets-lb-0-80572\n",
    "2. https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove\n",
    "3. https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle\n",
    "\n",
    "## Give me your feedback and if you find my kernel helpful please UPVOTE will be appreciated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
